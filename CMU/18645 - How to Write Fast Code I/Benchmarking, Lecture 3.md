To write fast code, we benchmark our machine (i.e. determine/measure our machine's capability):
- Latency of instructions: minimum # of cycles before next dependent instruction
- Instruction throughput: Instructions processed per clock cycle

Also note this is a very messy and time-consuming process. Runs on identical code at different times may yield drastically different answers. Getting accurate timings is difficult, so don't expect to have perfect answers and start benchmarking early.

One key assumption we make is that instructions are fully pipelined, meaning that for every cycle we can put in a new independent instruction.

Timing
![[Pasted image 20250903132140.png]]

We always think of a pipeline as one stage taking 1 cycle/clock tick and 5 instructions can be pipelined at a time. On realistic architectures however, the details of the pipeline are typically unknown and instead, we want to track the instruction's latency:
![[Pasted image 20250903132823.png]]

Latency example:
![[Pasted image 20250903133526.png]]
The issue with this is there's loop overheads like i++ in the timing and other instructions so it's not completely accurate. We need to eliminate the unnecessary loop through unrolling, so to properly measure the number of instructions in the time it takes to run, we would just have a chain of var++ and eliminate the loop completely.

Compiler difficulties
Even with the same instruction/code, you can have different latencies and performance due to different assembly generated by the compiler, so always check your assembly and make sure it lines up with what you expect:
![[Pasted image 20250903134326.png]]

One way we can set the correct assembly for our instructions is to declare it via C macros and write in macros in general:
![[Pasted image 20250903134842.png]]
![[Pasted image 20250903135049.png]]

So now, with a correct macro approach, this is how we would properly measure latency:
![[Pasted image 20250903135629.png]]

Extending to other instructions, the general process is to time a chain of dependent instructions then divide time by number of instructions in the chain.
- `rdtsc` may not be accurate for small cycle counts, so make sure your chains are between 100-1000s

Throughput is instructions per unit time (IPC = instructions per cycle). We are more concerned with IPC than CPI (cycles per instruction).
![[Pasted image 20250903135935.png]]
Also note that throughput is NOT 1/latency!

Throughput example:
![[Pasted image 20250903141659.png]]
If you have 3 independent chains of instructions (independent = different color), then you have a latency of 3 so you can process 3 instructions concurrently. However, since we only have a 3 cycle latency, if we introduce a fourth color then the second instruction of red and first instruction of green will conflict with each other. So, we need to stall red by one cycle so we can execute green. This leads to blue's 2nd instruction being delayed by one cycle to execute red's 2nd instruction and so on, creating the chain of one cycle stalls we see in the diagram.

So for the first three independent chains since it was a 3 cycle latency, a new chain increased execution time but by not much. But with the 4th chain, it does stall the pipeline a lot more by how long the chain is. All of this is also based on the assumption that the pipeline is able to be stalled by just one cycle like this.

![[Pasted image 20250903142831.png]]
![[Pasted image 20250903142845.png]]

question:
why are we assuming instructions are fully pipelined if we are talking abt measuring latency

Terms:
A functional unit of hardware is a component or a distinct part of a computer system designed to perform a specific task or set of operations, such as the arithmetic logic unit (ALU) for calculations, the memory unit for storage, or the control unit (CU) for coordinating other components. These units work together to process data and execute computer programs.